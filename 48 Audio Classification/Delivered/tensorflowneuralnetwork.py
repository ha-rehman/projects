# -*- coding: utf-8 -*-
"""TensorflowNeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJVZi1MTonJ-Xh7IXiNIG6evUsKTfmhr

**Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import os
import warnings
warnings.filterwarnings('ignore')

import IPython.display as ipd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn as skl
import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm
import librosa
import librosa.display
from sklearn.model_selection import train_test_split

import utils

plt.rcParams['figure.figsize'] = (17, 5)

"""**Loading Dataset**"""

# Directory where mp3 are stored.
AUDIO_DIR = os.environ.get('fma_small')

# Load metadata and features.
tracks = utils.load('fma_metadata/tracks.csv')
genres = utils.load('fma_metadata/genres.csv')
features = utils.load('fma_metadata/features.csv')
echonest = utils.load('fma_metadata/echonest.csv')

np.testing.assert_array_equal(features.index, tracks.index)
assert echonest.index.isin(tracks.index).all()

tracks.shape, genres.shape, features.shape, echonest.shape

"""**Spliting the train and test data into features and labels**"""

small = tracks['set', 'subset'] <= 'small'

train = tracks['set', 'split'] == 'training'
val = tracks['set', 'split'] == 'validation'
test = tracks['set', 'split'] == 'test'

y_train = tracks.loc[small & train, ('track', 'genre_top')]
y_test = tracks.loc[small & test, ('track', 'genre_top')]
X_train = features.loc[small & train, 'mfcc']
X_test = features.loc[small & test, 'mfcc']

print('{} training examples, {} testing examples'.format(y_train.size, y_test.size))
print('{} features, {} classes'.format(X_train.shape[1], np.unique(y_train).size))

"""**Randomly shuffling the train dataset for the best Results**"""

# Be sure training samples are shuffled.
X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=101)

# Standardize features by removing the mean and scaling to unit variance.
scaler = skl.preprocessing.StandardScaler(copy=False)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**Here is the y_Train and y_Test dataset distribution**"""

# Training data distribution
display(pd.Series(y_train).value_counts())

# Test data distribution
display(pd.Series(y_test).value_counts())

"""**Here is the number of records for the train and test dataset**"""

# Check the shape and data type of the data
print(X_train.shape)  # (num_samples, num_rows, num_columns, 1)
print(X_train.dtype)  # float32
print(X_test.shape)  # (num_samples, num_rows, num_columns, 1)
print(X_test.dtype)  # float32

"""**Converting into categorical**"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train)
y_test =to_categorical(y_test)

"""**Seperating the 10 persent dataset for the validation portion**"""

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=2)

# display shape of all data sets
print("Shape of y_train",y_train.shape)
print("Shape of y_test",y_test.shape)
print("Shape of y_test",y_val.shape)

"""**Applying the model for the classification**"""

# model development
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model=Sequential()
model.add(Dense(512,input_dim=140,activation='relu'))
model.add(Dense(256,activation='relu'))
model.add(Dense(128,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(16,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(128,activation='relu'))
model.add(Dense(256,activation='relu'))
model.add(Dense(512,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(8,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

"""**Using Early Stopping to stop over the best one model iteration**"""

from tensorflow.keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_accuracy', mode='max', patience=10, min_delta=.001)
history = model.fit(X_train,y_train,validation_data=(X_val,y_val), batch_size=64,epochs=50,verbose=1, callbacks=[es])

"""**Model TRaining Performance graph**"""

# Model Performance graph
plt.plot(history.history['accuracy'], label = "Accuracy")
plt.plot(history.history['loss'], label = "Loss")
plt.plot(history.history['val_loss'], label = "Validation loss")
plt.plot(history.history['val_accuracy'], label = "validation accuracy")
plt.title("Model Performance")
plt.xlabel("epochs")
plt.ylabel("Rate")
plt.legend()
plt.show()

"""**Model Accuracy Score over the test dataset**"""

# Test the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test loss: {loss:.4f}')
print(f'Test accuracy: {accuracy:.2f}')

"""**Model Evaluation Results over the test dataset**"""

from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, precision_score, recall_score,r2_score, mean_squared_error, f1_score

# function for evaluation metrics precision, recall, f1 etc
def modelEvaluation(predictions, y_test_set, model_name):
    # Print model evaluation to predicted result    
    print("==========",model_name,"==========")
    print ("\nAccuracy on validation set: {:.4f}".format(accuracy_score(y_test_set, predictions)))    
    print ("\nClassification report : \n", classification_report(y_test_set, predictions))
    print ("\nConfusion Matrix : \n", confusion_matrix(y_test_set, predictions))
    plt.figure(figsize=(10,10))
    sns.heatmap(confusion_matrix(y_test_set, predictions),annot=True, fmt='g',cmap='viridis')
    plt.tight_layout()
    plt.show()
    results = [accuracy_score(y_test_set, predictions), precision_score(y_test_set, predictions, average='macro'),
              recall_score(y_test_set, predictions, average='macro'),f1_score(y_test_set, predictions, average='macro')]
    return results

y_pred = model.predict(X_test)
y_actual = [np.argmax(i) for i in y_test]
pred = [np.argmax(i) for i in y_pred]
modelEvaluation(pred, y_actual, "Best MLP Tuned model")

