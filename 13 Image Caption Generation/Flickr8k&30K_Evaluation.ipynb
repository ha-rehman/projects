{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed378ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba9a96",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71ee6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c038711",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841dba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataset variable with 1 and 2 for flickr8k and flickr30k respectively\n",
    "dataset = 1\n",
    "max_length = 34 if dataset == 1 else 70\n",
    "# initialize the tokenizer for selected dataset\n",
    "tokenizer_path = 'tokenizer/flickr8k_tokenizer.pkl'\n",
    "image_path = 'image.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2bdd4",
   "metadata": {},
   "source": [
    "## VGG 16\n",
    "### utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87136898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "\t# load the model\n",
    "\tmodel = VGG19()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# load the photo\n",
    "\timage = load_img(filename, target_size=(224, 224))\n",
    "\t# convert the image pixels to a numpy array\n",
    "\timage = img_to_array(image)\n",
    "\t# reshape data for the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t# prepare the image for the VGG model\n",
    "\timage = preprocess_input(image)\n",
    "\t# get features\n",
    "\tfeature = model.predict(image, verbose=0)\n",
    "\treturn feature\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "\t# seed the generation process\n",
    "\tin_text = 'startseq'\n",
    "\t# iterate over the whole length of the sequence\n",
    "\tfor i in range(max_length):\n",
    "\t\t# integer encode input sequence\n",
    "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pad input\n",
    "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
    "\t\t# predict next word\n",
    "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
    "\t\t# convert probability to integer\n",
    "\t\tyhat = argmax(yhat)\n",
    "\t\t# map integer to word\n",
    "\t\tword = word_for_id(yhat, tokenizer)\n",
    "\t\t# stop if we cannot map the word\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\t# append as input for generating the next word\n",
    "\t\tin_text += ' ' + word\n",
    "\t\t# stop if we predict the end of the sequence\n",
    "\t\tif word == 'endseq':\n",
    "\t\t\tbreak\n",
    "\treturn in_text\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00187999",
   "metadata": {},
   "source": [
    "### load tokenizer, model & image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "# load the model\n",
    "model = load_model('models/flickr8k/vgg16/saved-model-05.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb464f11",
   "metadata": {},
   "source": [
    "### predict & show caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49b4046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq two children are playing in the grass endseq\n"
     ]
    }
   ],
   "source": [
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9934898",
   "metadata": {},
   "source": [
    "## VGG 19\n",
    "### utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd92092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "\t# load the model\n",
    "\tmodel = VGG19()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# load the photo\n",
    "\timage = load_img(filename, target_size=(224, 224))\n",
    "\t# convert the image pixels to a numpy array\n",
    "\timage = img_to_array(image)\n",
    "\t# reshape data for the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t# prepare the image for the VGG model\n",
    "\timage = preprocess_input(image)\n",
    "\t# get features\n",
    "\tfeature = model.predict(image, verbose=0)\n",
    "\treturn feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2c934",
   "metadata": {},
   "source": [
    "### load tokenizer, model & image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5cbc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('models/flickr8k/vgg19/saved-model-05.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd205b",
   "metadata": {},
   "source": [
    "### predict & show caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ff6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1c288",
   "metadata": {},
   "source": [
    "## InceptionV3\n",
    "### utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60001db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "def extract_features(filename):\n",
    "\t# load the model\n",
    "\tmodel = InceptionV3()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# load the photo\n",
    "\timage = load_img(filename, target_size=(299, 299))\n",
    "\t# convert the image pixels to a numpy array\n",
    "\timage = img_to_array(image)\n",
    "\t# reshape data for the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t# prepare the image for the VGG model\n",
    "\timage = preprocess_input(image)\n",
    "\t# get features\n",
    "\tfeature = model.predict(image, verbose=0)\n",
    "\treturn feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc82e93",
   "metadata": {},
   "source": [
    "### load tokenizer, model & image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open(tokenizer_path, 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('models/flickr8k/inceptionv3/saved-model-05.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497523f",
   "metadata": {},
   "source": [
    "### predict & show caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e347ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
