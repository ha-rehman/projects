{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29475a4a",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a990094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bb3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d87097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment class of game for configuration\n",
    "class ConnectFourEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.board_size = 4\n",
    "        self.board = np.zeros((self.board_size, self.board_size))\n",
    "        self.action_space = spaces.Discrete(self.board_size)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.board_size, self.board_size), dtype=np.float32)\n",
    "        self.current_player = 1\n",
    "        self.winner = None\n",
    "        self.done = False\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.get_obs(), 0, True, {}\n",
    "        \n",
    "        row = self._get_next_row(action)\n",
    "        if row is None:\n",
    "            return self.get_obs(), -10, False, {'player': self.current_player}\n",
    "        \n",
    "        self.board[row, action] = self.current_player\n",
    "        winner = self._check_for_winner()\n",
    "        if winner is not None:\n",
    "            reward = 10 if winner == 1 else -10\n",
    "            self.done = True\n",
    "            self.winner = winner\n",
    "        else:\n",
    "            reward = 2\n",
    "            self.current_player = 1 if self.current_player == -1 else -1\n",
    "            \n",
    "        return self.get_obs(), reward, self.done, {'player': self.current_player}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.board_size, self.board_size))\n",
    "        self.current_player = 1\n",
    "        self.winner = None\n",
    "        self.done = False\n",
    "        return self.get_obs()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(self.board)\n",
    "\n",
    "    def get_obs(self):\n",
    "        return self.board * self.current_player\n",
    "\n",
    "    def _get_next_row(self, action):\n",
    "        for row in range(self.board_size):\n",
    "            if self.board[row, action] == 0:\n",
    "                return row\n",
    "        return None\n",
    "\n",
    "    def _check_for_winner(self):\n",
    "        for row in range(self.board_size):\n",
    "            for col in range(self.board_size):\n",
    "                player = self.board[row, col]\n",
    "                if player == 0:\n",
    "                    continue\n",
    "                if col + 3 < self.board_size and np.all(self.board[row, col:col+4] == player):\n",
    "                    return player\n",
    "                if row + 3 < self.board_size and np.all(self.board[row:row+4, col] == player):\n",
    "                    return player\n",
    "                if col + 3 < self.board_size and row + 3 < self.board_size and np.all(np.diagonal(self.board[row:row+4, col:col+4]) == player):\n",
    "                    return player\n",
    "                if col + 3 < self.board_size and row - 3 >= 0 and np.all(np.diagonal(np.fliplr(self.board[row-3:row+1, col:col+4])) == player):\n",
    "                    return player\n",
    "        if np.count_nonzero(self.board) == self.board_size**2:\n",
    "            return 0\n",
    "        return None\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        # returns a list of valid actions for the current game state\n",
    "        valid_actions = []\n",
    "        for j in range(4):\n",
    "            if self.board[0][j] == 0:\n",
    "                valid_actions.append(j)\n",
    "        return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9db4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, learning_rate=0.001, discount_factor=0.99, epsilon=0.5, epsilon_decay=0.99, epsilon_min=0.01, batch_size=64, memory_size=100000):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(512, activation='relu', input_shape=(self.env.observation_space.shape[0] * self.env.observation_space.shape[1],)),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(8, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.env.action_space.n, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(lr=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon and self.epsilon > 0:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state.reshape(1, -1))[0])\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = np.array(random.sample(self.memory, self.batch_size))\n",
    "        states = np.stack(batch[:, 0]).reshape(self.batch_size, -1)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2]\n",
    "        next_states = np.stack(batch[:, 3]).reshape(self.batch_size, -1)\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        targets = self.model.predict(states)\n",
    "        q_values_next = self.model.predict(next_states)\n",
    "        max_q_values_next = np.amax(q_values_next, axis=1)\n",
    "        targets[np.arange(self.batch_size), actions] = rewards + (1 - dones) * self.discount_factor * max_q_values_next\n",
    "        self.model.fit(states, targets, batch_size=self.batch_size, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def train(self, episodes):\n",
    "        target_score = 30\n",
    "        episode = 0\n",
    "        best_score = -np.inf\n",
    "        while True:\n",
    "            episode += 1\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                self.replay()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            print(f\"Episode {episode} - Score: {score:.2f} - Best Score: {best_score:.2f} - Epsilon: {self.epsilon:.3f}\")\n",
    "            if best_score >= target_score:\n",
    "                print(f\"Target score of {target_score} reached after {episode} episodes\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb59c11",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b5cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\anaconda3\\envs\\tf2x\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "env = ConnectFourEnv()\n",
    "agent = DQN(env)\n",
    "if not os.path.exists('connect4_model.h5'):\n",
    "    agent.train(episodes=20)\n",
    "    agent.model.save('connect4_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbf9ca",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a51590",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('connect4_model.h5'):\n",
    "    scores = []\n",
    "    for i in range(20):\n",
    "        print(f\"Game {i+1}\")\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "        print(f\"Score: {score} \\n\")\n",
    "        scores.append(score)\n",
    "\n",
    "    print(f\"Average score over 100 games: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254d205",
   "metadata": {},
   "source": [
    "## Play with trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58fdbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from connect_four import ConnectFourEnv\n",
    "import time\n",
    "\n",
    "model1 = load_model('connect4_model1.h5')\n",
    "model2 = load_model('connect4_model2.h5')\n",
    "\n",
    "env = ConnectFourEnv()\n",
    "\n",
    "# Set up Pygame window\n",
    "pygame.init()\n",
    "FPS = 30\n",
    "WINDOW_WIDTH = 640\n",
    "WINDOW_HEIGHT = 480\n",
    "CELL_SIZE = 80\n",
    "BOARD_WIDTH = env.observation_space.shape[1]\n",
    "BOARD_HEIGHT = env.observation_space.shape[0]\n",
    "BOARD_OFFSET_X = int((WINDOW_WIDTH - BOARD_WIDTH * CELL_SIZE) / 2)\n",
    "BOARD_OFFSET_Y = int((WINDOW_HEIGHT - BOARD_HEIGHT * CELL_SIZE) / 2)\n",
    "WINDOW_SURF = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "pygame.display.set_caption('Connect Four')\n",
    "\n",
    "# Define colors\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "YELLOW = (255, 255, 0)\n",
    "\n",
    "# Define font\n",
    "FONT_SIZE = 32\n",
    "FONT = pygame.font.Font(None, FONT_SIZE)\n",
    "\n",
    "# Set up game variables\n",
    "state = env.reset()\n",
    "done = False\n",
    "player_turn = 1\n",
    "winner = None\n",
    "\n",
    "\n",
    "def draw_board(board):\n",
    "    for x in range(BOARD_WIDTH):\n",
    "        for y in range(BOARD_HEIGHT):\n",
    "            pygame.draw.rect(WINDOW_SURF, WHITE, (BOARD_OFFSET_X + x * CELL_SIZE, BOARD_OFFSET_Y + y * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "            if board[y][x] == 1:\n",
    "                pygame.draw.circle(WINDOW_SURF, YELLOW, (int(BOARD_OFFSET_X + (x + 0.5) * CELL_SIZE), int(BOARD_OFFSET_Y + (y + 0.5) * CELL_SIZE)), int(CELL_SIZE / 2.5))\n",
    "            elif board[y][x] == -1:\n",
    "                pygame.draw.circle(WINDOW_SURF, RED, (int(BOARD_OFFSET_X + (x + 0.5) * CELL_SIZE), int(BOARD_OFFSET_Y + (y + 0.5) * CELL_SIZE)), int(CELL_SIZE / 2.5))\n",
    "\n",
    "\n",
    "def draw_text(text, x, y, color):\n",
    "    text_surf = FONT.render(text, True, color)\n",
    "    text_rect = text_surf.get_rect()\n",
    "    text_rect.center = (x, y)\n",
    "    WINDOW_SURF.blit(text_surf, text_rect)\n",
    "\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    while not done:\n",
    "\n",
    "        # AI player 1's turn\n",
    "        q_values = model2.predict(state.reshape(1, -1))[0]\n",
    "        env.state = state\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        masked_q_values = np.ma.array(q_values,\n",
    "                                      mask=[not env.action_space.contains(i) for i in range(env.action_space.n)])\n",
    "        action = np.argmax(masked_q_values)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            winner = env.winner\n",
    "\n",
    "        WINDOW_SURF.fill(BLACK)\n",
    "        draw_board(state)\n",
    "        if winner:\n",
    "            draw_text(\"Agent {} wins!\".format(winner), int(WINDOW_WIDTH / 2), int(WINDOW_HEIGHT / 2),\n",
    "                      RED if winner == 1 else YELLOW)\n",
    "        else:\n",
    "            draw_text(\"Agent 1 turn\", int(WINDOW_WIDTH / 2), int(CELL_SIZE / 2), YELLOW)\n",
    "        pygame.display.update()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # AI player 2's turn\n",
    "        q_values = model1.predict(state.reshape(1, -1))[0]\n",
    "        env.state = state\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        masked_q_values = np.ma.array(q_values,\n",
    "                                      mask=[not env.action_space.contains(i) for i in range(env.action_space.n)])\n",
    "        action = np.argmax(masked_q_values)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            winner = env.winner\n",
    "\n",
    "        WINDOW_SURF.fill(BLACK)\n",
    "        draw_board(state)\n",
    "        if winner:\n",
    "            draw_text(\"Agent {} wins!\".format(winner), int(WINDOW_WIDTH / 2), int(WINDOW_HEIGHT / 2),\n",
    "                      RED if winner == 1 else YELLOW)\n",
    "        else:\n",
    "            draw_text(\"Agent 2 turn\", int(WINDOW_WIDTH / 2), int(CELL_SIZE / 2), RED)\n",
    "        pygame.display.update()\n",
    "        time.sleep(2)\n",
    "\n",
    "    time.sleep(2)\n",
    "    pygame.quit()\n",
    "\n",
    "# See PyCharm help at https://www.jetbrains.com/help/pycharm/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a103f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
