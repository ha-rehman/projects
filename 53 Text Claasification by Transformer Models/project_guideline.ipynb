{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ed61c3-9b01-4b0e-af10-a794e22b9f1e",
   "metadata": {},
   "source": [
    "In this notebook we try to explain in detail what we expect the project code / pipeline to look like. We put headings to indicate the different parts of the code (we took most headings from the code you already provided and added the parts that we additionally expect) to make clear how the project should look like.\n",
    "\n",
    "When it says \"code\" that is a placeholder for your code (which is already there). When we see that something is missing or unclear we wrote text.\n",
    "\n",
    "Please also read the \"Useful information\" part at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c483108-1762-4662-a7a3-bd5aeece020a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df23600-a4c9-44bf-9d32-76a68367150c",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74932f63-752c-4a37-8a0b-ab3019197f4e",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a50f7-4902-4775-bfb2-4d9415196afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0b5d4-6338-442b-8832-7c27ae82983a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6b184-158b-4889-99c2-6f24d9ec1bbe",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "Code\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Code\n",
    "\n",
    "## Encode Datasets\n",
    "\n",
    "Code\n",
    "\n",
    "## Process datasets\n",
    "\n",
    "Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260fe6b-dc1f-42d7-ac15-48d4e60674a2",
   "metadata": {},
   "source": [
    "# Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd6258-8657-456e-8806-3b41447465bd",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf14a3b-a823-4087-a5b7-7d77746c4bc7",
   "metadata": {},
   "source": [
    "### Code for\n",
    "**Create model without custom head, freeze the body**\n",
    "\n",
    "Please use this: https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification\n",
    "\n",
    "It should look something like this:\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=True)\n",
    "\n",
    "Here, the model head is automatically instanciated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa89c9f-8f96-4287-a837-8e818d80e901",
   "metadata": {},
   "source": [
    "### Code for\n",
    "**Create model with custom head, freeze the body**\n",
    "\n",
    "This model you already created for example in the first project, you can re-use this model, but extend it to freeze the body or use the model that you already created with CustomModel2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9749247a-0118-4970-b0cc-2c7d54ee4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self, checkpoint, num_labels): \n",
    "#         super(CustomModel,self).__init__() \n",
    "#         self.num_labels = num_labels \n",
    "\n",
    "#         #Load Model with given checkpoint and extract its body\n",
    "#         # Load Model with given checkpoint and extract its body\n",
    "#         config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True, output_attentions=True)\n",
    "#         self.model = AutoModel.from_pretrained(checkpoint, config=config)\n",
    "#         self.dropout = nn.Dropout(0.1) \n",
    "#         self.classifier = nn.Linear(768,num_labels) # set sequence length\n",
    "\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "#         #Extract outputs from the body\n",
    "#         outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "#         #Add custom layers\n",
    "#         sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "\n",
    "#         logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#           # set class weights here  \n",
    "#     #           device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     #           class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]).to(device) # set weights here\n",
    "#           loss_fct = nn.CrossEntropyLoss()\n",
    "#           loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "#         return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31aee4a-664f-4773-90c0-e351301497de",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "Please note that in CustomModel1 and CustomModel2 this \"pretrain_model = BertForPreTraining.from_pretrained(checkpoint, config=config)\" from the CustomModel class is not correct. It should be AutoModel or AutoModelForSequenceClassification when using the automatic head from hugging face.\n",
    "\n",
    "https://stackoverflow.com/questions/66596142/bertmodel-or-bertforpretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e27c7-065c-48ca-a932-32ddef73ef43",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aede9-57f4-4983-856d-fef78afc0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2ae3c-de7a-4de2-81ca-1dc3e17cd86e",
   "metadata": {},
   "source": [
    "### Evaluation and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921f981-9221-4b20-ab8c-c3558036d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a3478-d347-4655-8916-4344ab84e0fa",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ef584-e15d-4511-bad7-f0fa8ef7b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7464f0-6834-4259-8dcd-a76b02290c93",
   "metadata": {},
   "source": [
    "### Configurations and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8a442-4d5a-4783-9ca4-28aa1481dd1f",
   "metadata": {},
   "source": [
    "Here the code to start training for the model with custom head and the model without custom head. Below is what you have provided so far, maybe that is sufficient, maybe you have to adjust, not sure. \n",
    " \n",
    "One question: please add comments behind the lines of the code, how the code knows that it must run the model without custom head first and then the model with custom head. Please highlight that in the code.\n",
    "\n",
    "The data must be plit into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f91a12-76fe-4d44-972a-fcafb117e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models_file = 'trained_models.txt' #WHAT DOES THIS DO?\n",
    "best_params_dict_path = 'best_params.json' #WHAT DOES THIS DO?\n",
    "\n",
    "df = pickle.load(open('dataset/230130_SmallOberkategorie.pickle', 'rb'))\n",
    "label_names = df['labels'].unique()\n",
    "# label_names\n",
    "\n",
    "X = df[['text']]\n",
    "y = df['labels']\n",
    "X_train, X_val, y_train, y_val = split_df(X, y)\n",
    "\n",
    "custom_head = False\n",
    "freeze_layers = True\n",
    "optimizers = [Adam]\n",
    "learning_rates = [2e-6]\n",
    "epochs = [2]\n",
    "batch_sizes = [16]\n",
    "\n",
    "configs =     {'custom_head': custom_head, 'freeze_layers': freeze_layers,\n",
    "               'optimizers': optimizers, 'epochs': epochs, 'batch_sizes': batch_sizes,\n",
    "               'learning_rates': learning_rates, 'val_steps':100}\n",
    "dataset = {'X_train': X_train, 'y_train': y_train, 'X_val': X_val, 'y_val': y_val}\n",
    "\n",
    "models = [\n",
    "    'Bert-base-german-cased', \n",
    "'Dbmdz/bert-base-german-uncased',\n",
    "'Deepset/gbert-base',\n",
    "'Xlm-roberta-base',\n",
    "'Uklfr/gottbert-base'\n",
    "]\n",
    "\n",
    "# training without custom head\n",
    "training(models, configs, dataset)\n",
    "\n",
    "# training with custom head\n",
    "configs['custom_head'] = True\n",
    "training(models, configs, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51428d1f-28db-443c-a005-071633881276",
   "metadata": {},
   "source": [
    "**Expected results:**\n",
    "\n",
    "For each of the 5 models should be two models/weights saved:\n",
    " * 5 models without custom head\n",
    " * 5 models with custom head\n",
    " \n",
    " \n",
    " **Expected Output**\n",
    " \n",
    " The expected output regarding metrics and graphs is the same as it is now, no changes needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d1c88-d2d1-4a0e-a206-535b87d081eb",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147b14a-7f67-4583-be4c-f33d41d2ccbb",
   "metadata": {},
   "source": [
    "Code to basically do the same that was done during Fine-tuning EXCEPT that now, we want to use the saved fine-tuned models. So instead of creating custom models again, we just want to load the saved models (model after model, not all at once) and train them with *different hyperparameter setting*. The best weights for each model should be saved (not overwrite savings from fine-tuning).\n",
    "\n",
    "We also want to load data (a different dataset than during fine-tuning. Data must be split into train and test), we also want to see the same outputs (metrics, graphs etc. as after fine-tuning). Probably the code you already have can be re-used and just minor changes must be made.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf6c70-06b8-4688-9a6b-ef543cb1bc6d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650eb423-2a06-4ba6-89ec-68efc1907462",
   "metadata": {},
   "source": [
    "Code for testing the saved models (10 in total) from the hyperparameter-tuning step. The same hyperparameter configurations should be used that gave the best results during hyperparameter tuning.\n",
    "\n",
    "The rest is as it was already communicated (mismatched data, attention weights etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8adc1-c01f-4786-87c6-5fb5a4fcc81f",
   "metadata": {},
   "source": [
    "# Useful information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d57811-a9f2-439b-b1ff-62a9d29875b6",
   "metadata": {},
   "source": [
    "**What is Model Fine-Tuning?**\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. So, training a BERT model from scratch on a small dataset would result in overfitting.\n",
    "\n",
    "So, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning.\n",
    "\n",
    "**Different Fine-Tuning Techniques**\n",
    "* 1) Train the entire architecture – We can further train the entire pre-trained model on our dataset and feed the output to a softmax layer. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new dataset.\n",
    "* 2) Train some layers while freezing others – Another way to use a pre-trained model is to train it partially. What we can do is keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained.\n",
    "* 3) Freeze the entire architecture – We can even freeze all the layers of the model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "\n",
    "\n",
    "**OUR GOAL**\n",
    "\n",
    "We basically want to freeze the backbone (body) and only train the last layers (classifier). Then we want to unfreeze (hyperparametertuning) the model and train all of it while also searching for the best hyperparameter options.\n",
    "\n",
    "We want to see, if for our task this approach performs better than direct training without freezing the body layers.\n",
    "\n",
    "So it would be helpful, if in a separate notebook, the same code pipeline is delivered WITHOUT the fine-tuning step in the beginning and directly starting with the hyperparameter-tuning / training part of CustomModel1 and CustomModel2 (model without custom head, model with custom head).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beee1a6-d947-44e7-927c-04e3ba73e8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
